{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVD 5. cvičení\n",
    "\n",
    "## 1. část - TF-IDF s word embeddingy\n",
    "\n",
    "V minulém cvičení bylo za úkol implementovat TF-IDF algoritmus nad datasetem z Kagglu. Dnešní cvičení je rozšířením této úlohy s použitím word embeddingů. Lze použít předtrénované GloVe embeddingy ze 3. cvičení, nebo si v případě zájmu můžete vyzkoušet práci s Word2Vec od Googlu (najdete [zde](https://code.google.com/archive/p/word2vec/)).\n",
    "\n",
    "Cvičení by mělo obsahovat následující části:\n",
    "- Načtení článků a embeddingů\n",
    "- Výpočet document vektorů pomocí TF-IDF a word embeddingů \n",
    "    - Pro výpočet TF-IDF využijte [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) z knihovny sklearn\n",
    "    - Vážený průměr GloVe / Word2Vec vektorů\n",
    "\n",
    "$$ doc\\_vector = \\frac{1}{|d|} \\sum\\limits_{w \\in d} TF\\_IDF(w) glove(w) $$\n",
    "\n",
    "$w$ ... slovo<br>\n",
    "$d$ ... dokument<br>\n",
    "\n",
    "- Dotaz bude transformován stejně jako dokument\n",
    "\n",
    "- Výpočet relevance pomocí kosinové podobnosti\n",
    "\n",
    "$$ score(q,d) = cos\\_sim(query\\_vector, doc\\_vector) $$\n",
    "\n",
    "$q$ ... dotaz<br>\n",
    "$d$ ... dokument<br>\n",
    "\n",
    "### Načtení článků"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pytel/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pytel/.local/lib/python3.10/site-packages/spacy/language.py:1895: UserWarning: [W123] Argument disable with value ['parser', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/articles.csv\"\n",
    "lemmatizer = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as read_obj:\n",
    "    dict_reader = csv.DictReader(read_obj)\n",
    "    data_raw = list(dict_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purge_chars(text, chars_to_remove):\n",
    "    for c in list(chars_to_remove):\n",
    "        text = text.replace(c, \"\")\n",
    "    return text\n",
    "\n",
    "def prep_text(text):\n",
    "    # Převést všechen text na lower case\n",
    "    text = text.lower()\n",
    "    # Odstranění interpunkce a všech speciálních znaků (apostrof, ...)\n",
    "    numbers = \"1234567890\"\n",
    "    interpunction = \",.:;?!\"\n",
    "    chars = numbers + interpunction + '#^&@$€Łłþ→ø%+*/|\\–—-\\'’‘\"\"”[]{}()'\n",
    "    text = purge_chars(text, chars)\n",
    "    # remove white spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # Aplikace lemmatizátoru:\n",
    "    return ' '.join([token.lemma_ for token in lemmatizer(text)])\n",
    "\n",
    "def process_data(data):\n",
    "    \"\"\"\n",
    "    data :\n",
    "    [[Title, Text], ...] ->\n",
    "    [[title, [word, ...]], ...]\n",
    "    \"\"\"\n",
    "    ret = np.empty([len(data),2], dtype=object)\n",
    "    for index, article in enumerate(data):\n",
    "        ret[index, 0] = prep_text(article[\"title\"])\n",
    "        ret[index, 1] = prep_text(article[\"text\"])\n",
    "    return ret\n",
    "\n",
    "def split_to_word_lists(texts):\n",
    "    return [text.split(' ') for text in texts]\n",
    "\n",
    "def make_reverse_indexing(texts):\n",
    "    words_indexes = {}\n",
    "    word_lists = split_to_word_lists(texts)\n",
    "    for index, words_list in enumerate(word_lists):\n",
    "        #print(words_list)\n",
    "        for word in words_list:\n",
    "            indexes = words_indexes.get(word, [])\n",
    "            indexes.append(index)\n",
    "            words_indexes[word] = indexes\n",
    "            #print(\"words_indexes[\", word, \"] ->\", indexes)\n",
    "    return words_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data(data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Načtení embeddingů"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "SIZES = [50, 100, 200, 300]\n",
    "DIRECTORY = \"data\"\n",
    "FILE_NAME = \"glove.6B.\"\n",
    "path = DIRECTORY + \"/\" + FILE_NAME + str(SIZES[0]) + \"d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    word2idx = {}\n",
    "    words = []\n",
    "    vectors = []\n",
    "    with open(file_name, \"r\") as file:\n",
    "        for i, line in enumerate(file.readlines()):\n",
    "            key, *values = line.strip().split(\" \")\n",
    "            vector = np.array([float(number) for number in values])\n",
    "            words.append(key)\n",
    "            vectors.append(vector)\n",
    "            word2idx[key] = i\n",
    "            if 0 and DEBUG:\n",
    "                print(key)\n",
    "                print(vector)\n",
    "        \n",
    "    return np.array(words), np.array(vectors), word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "words, vectors, word2idx = load_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + Word2Vec a vytvoření doc vektorů\n",
    "\n",
    "- Funguje lépe při menším množství dat\n",
    "- Průměrujeme vážené Word2Vec vektory\n",
    "- Váhou každého slova je získané TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_columm(array, columm=0, reverse_=False):\n",
    "    return(sorted(array, key = lambda x: x[columm], reverse=reverse_)) \n",
    "\n",
    "def unique(array):\n",
    "    return list(set(array))\n",
    "\n",
    "def document_frequency(word, inverse_index):\n",
    "    return len(unique(inverse_index.get(word, set())))\n",
    "\n",
    "def c(word, inverse_index, document_index):\n",
    "    return inverse_index.get(word, []).count(document_index)\n",
    "\n",
    "def tf_idf(word, inverse_index, texts, document_index):\n",
    "    M = len(texts)\n",
    "    c2 = c(word, inverse_index, document_index)\n",
    "    return c2 * np.log((M+1)/document_frequency(word, inverse_index))\n",
    "\n",
    "def phrase_vector(words, vectors, word2idx, inverse_index, texts, document_index):\n",
    "    vector = np.zeros(vectors[0].shape)\n",
    "    for word in words:\n",
    "        index = word2idx.get(word, None)\n",
    "        if index is not None:\n",
    "            vector += vectors[index] * tf_idf(word, inverse_index, texts, document_index)\n",
    "    return vector/len(words)\n",
    "\n",
    "def generate_phrase_vectors(vectors, word2idx, inverse_index, texts):\n",
    "    ret = []\n",
    "    for index, text in enumerate(texts):\n",
    "        words = text.split(' ')\n",
    "        vector = phrase_vector(words, vectors, word2idx, inverse_index, texts, index)\n",
    "        ret.append(vector)\n",
    "    return np.array(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flatten' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[39m=\u001b[39m unique(flatten(split_to_word_lists(titles)))\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words:\n\u001b[1;32m      3\u001b[0m     number \u001b[39m=\u001b[39m df(word, inverse_index)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'flatten' is not defined"
     ]
    }
   ],
   "source": [
    "words = unique(flatten(split_to_word_lists(titles)))\n",
    "for word in words:\n",
    "    number = df(word, inverse_index)\n",
    "    if number == 0:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data\n",
    "titles = documents[:,0]\n",
    "texts = documents[:,1]\n",
    "title_indexing = make_reverse_indexing(titles)\n",
    "text_indexing = make_reverse_indexing(texts)\n",
    "\n",
    "title_vector = generate_phrase_vectors(vectors, word2idx, title_indexing, titles)\n",
    "doc_vector = generate_phrase_vectors(vectors, word2idx, text_indexing, texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformace dotazu a výpočet relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(w1, w2):\n",
    "    if len(w1.shape) < 2:\n",
    "        w1 = w1.reshape(1, -1)\n",
    "    return (np.dot(w1, w2)) / (np.linalg.norm(w1, axis=1) * np.linalg.norm(w2) + 0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef score(query, vectors, word2idx, title_vector, doc_vector, texts, alpha=0.7):\\n    words = prep_text(query).split(' ')\\n    phrase_vector_query(words, vectors, word2idx, inverse_index, texts)\\n    return alpha*tf_idf_title + (1-alpha)*tf_idf_text\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tf_idf_query(word, words, inverse_index, texts):\n",
    "    M = len(texts)\n",
    "    c1 = words.count(word)\n",
    "    return c1 * np.log((M+1)/document_frequency(word, inverse_index))\n",
    "\n",
    "def phrase_vector_query(words, vectors, word2idx, inverse_index, texts):\n",
    "    vector = np.zeros(vectors[0].shape)\n",
    "    for word in words:\n",
    "        index = word2idx.get(word, None)\n",
    "        if index is not None:\n",
    "            vector += vectors[index] * tf_idf_query(word, words, inverse_index, texts)\n",
    "    return vector/len(words)\n",
    "\n",
    "\"\"\"\n",
    "def score(query, vectors, word2idx, title_vector, doc_vector, texts, alpha=0.7):\n",
    "    words = prep_text(query).split(' ')\n",
    "    phrase_vector_query(words, vectors, word2idx, inverse_index, texts)\n",
    "    return alpha*tf_idf_title + (1-alpha)*tf_idf_text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(337, 50)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "#query = \"coursera vs udacity machine learning\"\n",
    "query = \"defense skepticism deep learning\"\n",
    "alpha=0.7\n",
    "#scores = score(query, title_vector, doc_vector, texts)\n",
    "words = prep_text(query).split(' ')\n",
    "query_title_vector = phrase_vector_query(words, vectors, word2idx, title_indexing, texts)\n",
    "query_text_vector = phrase_vector_query(words, vectors, word2idx, text_indexing, titles)\n",
    "print(title_vector.shape)\n",
    "print(query_title_vector.shape)\n",
    "scores = alpha*similarity(title_vector, query_title_vector) + (1-alpha)*similarity(doc_vector, query_text_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores.shape)\n",
    "print(scores)\n",
    "print(np.argsort(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.argsort(scores, )[::-1]\n",
    "print(indexes)\n",
    "scores_sorted = scores[indexes]\n",
    "print(scores_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: \t Title: \t Score:\n",
      "209 \t in defense of skepticism about deep learning gary marcus medium \t 0.791487775796834\n",
      "327 \t in defense of skepticism about deep learning gary marcus medium \t 0.791487775796834\n",
      "135 \t obamarnn machine generate political speech samim medium \t 0.7381373562012444\n",
      "75 \t artificial intelligence the revolution have not happen yet \t 0.7336936082759244\n",
      "100 \t artificial intelligence the revolution have not happen yet \t 0.7336936082759244\n",
      "20 \t artificial intelligence the revolution have not happen yet \t 0.7336936082759244\n",
      "37 \t the current state of machine intelligence shivon zilis medium \t 0.7333870109084172\n",
      "117 \t the current state of machine intelligence shivon zilis medium \t 0.7333870109084172\n",
      "210 \t how to learn deep learning in month towards data science \t 0.7280966545787406\n",
      "15 \t machine learning be fun part modern face recognition with deep learning \t 0.7123480770679826\n",
      "57 \t machine learning be fun part modern face recognition with deep learning \t 0.7123480770679826\n",
      "170 \t machine learning be fun part modern face recognition with deep learning \t 0.7123480770679826\n",
      "41 \t what make a good data scientistengineer towards data science \t 0.7111731674251833\n",
      "195 \t simple reinforcement learning with tensorflow part deep qnetwork and beyond \t 0.7090319040560065\n",
      "116 \t use artificial intelligence to balance out customer value \t 0.7043779936347153\n",
      "183 \t the state of artificial intelligence in six visual \t 0.702873377160169\n",
      "246 \t reason why now be the time for artificial intelligence \t 0.7005351811123579\n",
      "165 \t the good word towards data science \t 0.6983483438234743\n",
      "333 \t artificial intelligence ai in and beyond towards data science \t 0.6980344096525515\n",
      "7 \t the big list of dsml interview resource towards data science \t 0.6980027164677339\n",
      "91 \t the big list of dsml interview resource towards data science \t 0.6980027164677339\n",
      "113 \t be there a future for innovation become human artificial intelligence magazine \t 0.6974361390076221\n",
      "233 \t be there a future for innovation become human artificial intelligence magazine \t 0.6974361390076221\n",
      "266 \t deep learning achievement over the past year stat and bot \t 0.6974302256029932\n",
      "12 \t deep learning be go to teach we all the lesson of our life job be for machine \t 0.6927303880359996\n",
      "138 \t machine learning be fun part how to do speech recognition with deep learning \t 0.692126704057538\n",
      "60 \t machine learning be fun part how to do speech recognition with deep learning \t 0.692126704057538\n",
      "218 \t blockchain be not only crappy technology but a bad vision for the future \t 0.691912361505884\n",
      "224 \t blockchain be not only crappy technology but a bad vision for the future \t 0.691912361505884\n",
      "152 \t what worry I about ai françois chollet medium \t 0.6917715255493156\n",
      "262 \t thought after take the deeplearningai course towards data science \t 0.6892485989071253\n",
      "229 \t thought after take the deeplearningai course towards data science \t 0.6892485989071253\n",
      "204 \t thought after take the deeplearningai course towards data science \t 0.6892485989071253\n",
      "85 \t gamechange trend to look out for with ai wisewolf fund medium \t 0.6880374464279125\n",
      "231 \t reason why your neural network be not work slav \t 0.6878521431118378\n",
      "264 \t reason why your neural network be not work slav \t 0.6878521431118378\n",
      "321 \t reason why your neural network be not work slav \t 0.6878521431118378\n",
      "140 \t eleven reason to be excited about the future of technology \t 0.6867521995370892\n",
      "119 \t how artificial intelligence can improve online news \t 0.6858633987519318\n",
      "289 \t how to use noise to your advantage towards data science \t 0.6850325810571957\n",
      "281 \t how to use noise to your advantage towards data science \t 0.6850325810571957\n",
      "251 \t an augmentation base deep neural network approach to learn human drive behavior \t 0.683384381925759\n",
      "302 \t how to create a mind the secret of human thought reveal \t 0.6775251795765528\n",
      "108 \t become a cyborg should be take gently of modern biopaleomachine cyborgology \t 0.6738812224847401\n",
      "111 \t new to machine learning avoid these three mistake \t 0.6734634445360764\n",
      "30 \t new to machine learning avoid these three mistake \t 0.6734634445360764\n",
      "283 \t simple reinforcement learning with tensorflow part qlearne with table and neural network \t 0.670407007874982\n",
      "171 \t simple reinforcement learning with tensorflow part qlearne with table and neural network \t 0.670407007874982\n",
      "291 \t simple reinforcement learning with tensorflow part qlearne with table and neural network \t 0.670407007874982\n",
      "308 \t simple reinforcement learning with tensorflow part qlearne with table and neural network \t 0.670407007874982\n",
      "59 \t simple reinforcement learning with tensorflow part qlearne with table and neural network \t 0.670407007874982\n",
      "137 \t simple reinforcement learning with tensorflow part qlearne with table and neural network \t 0.670407007874982\n",
      "207 \t explain simply how an ai program master the ancient game of go \t 0.6691837900355537\n",
      "154 \t explain simply how an ai program master the ancient game of go \t 0.6691837900355537\n",
      "78 \t how to solve of nlp problem a stepbystep guide \t 0.668125047784703\n",
      "134 \t how artificial intelligence will make technology disappear \t 0.6658533404589471\n",
      "123 \t ai be come and it will be bore denny vrandečić medium \t 0.6653380743360848\n",
      "225 \t a brief history of cnn in image segmentation from rcnn to mask rcnn \t 0.6635429028967876\n",
      "259 \t a brief history of cnn in image segmentation from rcnn to mask rcnn \t 0.6635429028967876\n",
      "172 \t a brief history of cnn in image segmentation from rcnn to mask rcnn \t 0.6635429028967876\n",
      "198 \t a brief history of cnn in image segmentation from rcnn to mask rcnn \t 0.6635429028967876\n",
      "71 \t a brief history of cnn in image segmentation from rcnn to mask rcnn \t 0.6635429028967876\n",
      "121 \t why natural search be awesome and how we get here the vanguard medium \t 0.6628803188157699\n",
      "103 \t interview with google alfred spector on voice search hybrid intelligence and more \t 0.6612493852591417\n",
      "22 \t interview with google alfred spector on voice search hybrid intelligence and more \t 0.6612493852591417\n",
      "335 \t surprise neuron be now more complex than we think \t 0.6595403935784997\n",
      "336 \t \" wth do a neural network even learn a newcomer dilemma \t 0.6593437032604615\n",
      "47 \t machine learning in a week learn new stuff medium \t 0.6582763885515122\n",
      "93 \t what I learn from interview at multiple ai company and startup \t 0.6540829458553359\n",
      "160 \t what I learn from interview at multiple ai company and startup \t 0.6540829458553359\n",
      "9 \t what I learn from interview at multiple ai company and startup \t 0.6540829458553359\n",
      "280 \t humanlike machine hearing with ai towards data science \t 0.6531152678075804\n",
      "147 \t the impossibility of intelligence explosion françois chollet medium \t 0.6521145022429554\n",
      "45 \t build the interest platform pinter engineering medium \t 0.6494098847818647\n",
      "32 \t reinvent social science in the era of big datum I love experiment medium \t 0.6482836623313238\n",
      "223 \t chatbots could we talk aima ai marketing magazine medium \t 0.6463013857449924\n",
      "227 \t be programmer head toward another bursting bubble \t 0.6460892839238906\n",
      "261 \t be programmer head toward another bursting bubble \t 0.6460892839238906\n",
      "230 \t the rise of the weaponize ai propaganda machine scout science fiction journalism medium \t 0.6458968975657796\n",
      "263 \t the rise of the weaponize ai propaganda machine scout science fiction journalism medium \t 0.6458968975657796\n",
      "329 \t a \" weird introduction to deep learning towards data science \t 0.6451648047351841\n",
      "254 \t the future of work oxford university medium \t 0.6447047846719288\n",
      "243 \t I think I m slowly turn into a cyborg become human artificial intelligence magazine \t 0.641939650505401\n",
      "153 \t who be go to make money in ai part I towards data science \t 0.6414594651579767\n",
      "83 \t who be go to make money in ai part I towards data science \t 0.6414594651579767\n",
      "272 \t how I launch six side project in hacker noon \t 0.6406484173251594\n",
      "161 \t how to easily automate dronebased monitoring use deep learning \t 0.6405209184256405\n",
      "211 \t the trick that make alphago zero work hacker noon \t 0.6400104011994562\n",
      "236 \t reflection of \" her ec mccarthy medium \t 0.6396417241316726\n",
      "301 \t everything you need to know about artificial neural network \t 0.6366553170319083\n",
      "51 \t the unreasonable effectiveness of random forest rant on machine learning medium \t 0.6362929153401542\n",
      "282 \t deep learning tip and trick towards data science \t 0.6350803109078459\n",
      "144 \t a beginner guide to aiml 🤖 👶 machine learn for human medium \t 0.634978636553236\n",
      "68 \t a beginner guide to aiml 🤖 👶 machine learn for human medium \t 0.634978636553236\n",
      "196 \t a beginner guide to aiml 🤖 👶 machine learn for human medium \t 0.634978636553236\n",
      "331 \t stochastic weight average a new way to get state of the art result in deep learning \t 0.6343345905352955\n",
      "92 \t must know information theory concept in deep learning ai \t 0.634319654877951\n",
      "8 \t must know information theory concept in deep learning ai \t 0.634319654877951\n",
      "159 \t must know information theory concept in deep learning ai \t 0.634319654877951\n",
      "163 \t use deep qlearning in fifa to perfect the art of freekick \t 0.6338087563026721\n",
      "290 \t the complex language use in back propagation kelvin li medium \t 0.6330313815856193\n",
      "188 \t invest in artificial intelligence nathan benaich medium \t 0.6329285054459769\n",
      "38 \t architecte a machine learning system for risk airbnb engineering data science medium \t 0.631790038347869\n",
      "192 \t ultimate guide to leverage nlp machine learning for your chatbot \t 0.6298915517958662\n",
      "64 \t machine learning in a year learn new stuff medium \t 0.6287637434583088\n",
      "212 \t how we grow from to million woman on our fashion app with a vertical machine learning approach \t 0.6286867676810252\n",
      "114 \t I schedule meeting in and that do not count reschedule xai \t 0.6284329429042417\n",
      "48 \t what to do with \" small datum rant on machine learning medium \t 0.6281791317713619\n",
      "3 \t machine learn how to go from zero to hero freecodecamp \t 0.6245063146565969\n",
      "215 \t machine learn how to go from zero to hero freecodecamp \t 0.6245063146565969\n",
      "156 \t machine learn how to go from zero to hero freecodecamp \t 0.6245063146565969\n",
      "279 \t datum augmentation how to use deep learning when you have limit datum part \t 0.6241850303254143\n",
      "17 \t the mindblowing ai announcement from google that you probably miss \t 0.6224839374149945\n",
      "97 \t the mindblowing ai announcement from google that you probably miss \t 0.6224839374149945\n",
      "66 \t the mindblowing ai announcement from google that you probably miss \t 0.6224839374149945\n",
      "142 \t the mindblowing ai announcement from google that you probably miss \t 0.6224839374149945\n",
      "88 \t reinforcement learning from scratch insight datum \t 0.6220654526885142\n",
      "157 \t reinforcement learning from scratch insight datum \t 0.6220654526885142\n",
      "4 \t reinforcement learning from scratch insight datum \t 0.6220654526885142\n",
      "109 \t why you just can not black box an ai weird thing \t 0.6214929879324862\n",
      "185 \t baidu explain how its master mandarin with deep learning \t 0.6213749136858665\n",
      "76 \t do algorithm reveal sexual orientation or just expose our stereotype \t 0.6207301588877325\n",
      "151 \t do algorithm reveal sexual orientation or just expose our stereotype \t 0.6207301588877325\n",
      "228 \t do algorithm reveal sexual orientation or just expose our stereotype \t 0.6207301588877325\n",
      "205 \t do algorithm reveal sexual orientation or just expose our stereotype \t 0.6207301588877325\n",
      "1 \t python for datum science concept you may have forget \t 0.6206115152278168\n",
      "54 \t generate story about image samim medium \t 0.6202572529330265\n",
      "247 \t what could happen if we do thing right an interview with kim stanley robinson author of aurora \t 0.620224234871628\n",
      "257 \t how to create a chatbot without code a single line \t 0.6192713793872677\n",
      "62 \t deep learn the stock market tal perry medium \t 0.6183436345337083\n",
      "141 \t deep learn the stock market tal perry medium \t 0.6183436345337083\n",
      "189 \t deep learn the stock market tal perry medium \t 0.6183436345337083\n",
      "191 \t how to build a recurrent neural network in tensorflow \t 0.6181267516102558\n",
      "143 \t every single machine learning course on the internet rank by your review \t 0.618017877482726\n",
      "19 \t every single machine learning course on the internet rank by your review \t 0.618017877482726\n",
      "67 \t every single machine learning course on the internet rank by your review \t 0.618017877482726\n",
      "99 \t every single machine learning course on the internet rank by your review \t 0.618017877482726\n",
      "105 \t the evolve role of business analytic frank diana medium \t 0.6163424759466233\n",
      "39 \t from wordvec to docvec an approach drive by chinese restaurant process \t 0.6159719389946737\n",
      "299 \t from wordvec to docvec an approach drive by chinese restaurant process \t 0.6159719389946737\n",
      "46 \t algorithm of the mind deep learning medium \t 0.6155132108528495\n",
      "275 \t you can build a neural network in javascript even if you do not really understand neural network \t 0.6152866534592678\n",
      "332 \t you can build a neural network in javascript even if you do not really understand neural network \t 0.6152866534592678\n",
      "182 \t into the age of context cross the pond medium \t 0.6150775186509322\n",
      "220 \t artificial intelligence top article june \t 0.6148481438199477\n",
      "324 \t you request someone with a degree in this hold up hand \t 0.614161997179932\n",
      "208 \t the fall of rnn lstm towards data science \t 0.6139651493842391\n",
      "84 \t the fall of rnn lstm towards data science \t 0.6139651493842391\n",
      "248 \t the internet of thing and the operating room of the future \t 0.6137518387030317\n",
      "316 \t write an ai to win at pong from scratch with reinforcement learning \t 0.610600712200459\n",
      "274 \t the future we want leigh alexander medium \t 0.6096630233581621\n",
      "16 \t I interview at five top company in silicon valley in five day and luckily get five job offer \t 0.6094378694244844\n",
      "65 \t I interview at five top company in silicon valley in five day and luckily get five job offer \t 0.6094378694244844\n",
      "173 \t a simple deep learning model for stock price prediction use tensorflow \t 0.6087546796340209\n",
      "73 \t a simple deep learning model for stock price prediction use tensorflow \t 0.6087546796340209\n",
      "199 \t a simple deep learning model for stock price prediction use tensorflow \t 0.6087546796340209\n",
      "325 \t a guide for time series prediction use recurrent neural network lstms \t 0.6077988913300199\n",
      "267 \t what be the good intelligent chatbot or ai chatbot available online \t 0.6077139857617763\n",
      "86 \t chatbots be the next big thing what happen the startup medium \t 0.605763784121977\n",
      "0 \t chatbots be the next big thing what happen the startup medium \t 0.605763784121977\n",
      "273 \t chatbots be the next big thing what happen the startup medium \t 0.605763784121977\n",
      "312 \t learn how to code neural network learn new stuff medium \t 0.6057423188232052\n",
      "110 \t why do we want to build a fully fledge agi weird thing \t 0.6052392571935865\n",
      "35 \t travel santa problem an incompetent algorist attempt \t 0.6040319522556361\n",
      "194 \t neural network for algorithmic trading simple time series forecasting \t 0.6012891475806488\n",
      "61 \t machine learning be fun part language translation with deep learning and the magic of sequence \t 0.6011683063370914\n",
      "139 \t machine learning be fun part language translation with deep learning and the magic of sequence \t 0.6011683063370914\n",
      "162 \t how to build your own neural network from scratch in python \t 0.6001118029310055\n",
      "133 \t digital transformation of business and society frank diana medium \t 0.5996236172276731\n",
      "87 \t artificial intelligence be humanitys rorschach test \t 0.5994182584542324\n",
      "216 \t what be \" q from a laymen coinmonk medium \t 0.5977817486865853\n",
      "241 \t the wtf economy from the wtf economy to the next economy \t 0.5971852833302628\n",
      "300 \t how to build a multilayere neural network in python \t 0.5949588557103034\n",
      "131 \t how to build a multilayere neural network in python \t 0.5949588557103034\n",
      "179 \t how to build a multilayere neural network in python \t 0.5949588557103034\n",
      "330 \t the new neural internet be come hacker noon \t 0.5929272039735449\n",
      "81 \t the cluster algorithms datum scientist need to know \t 0.5921159659528765\n",
      "107 \t emotional computing robbie tilton medium \t 0.590497992995263\n",
      "28 \t emotional computing robbie tilton medium \t 0.590497992995263\n",
      "181 \t video of a neural network learn deep learning medium \t 0.589292142055066\n",
      "309 \t yes you should understand backprop andrej karpathy medium \t 0.5888676981997754\n",
      "190 \t yes you should understand backprop andrej karpathy medium \t 0.5888676981997754\n",
      "293 \t yes you should understand backprop andrej karpathy medium \t 0.5888676981997754\n",
      "63 \t yes you should understand backprop andrej karpathy medium \t 0.5888676981997754\n",
      "258 \t how invisible interface be go to transform the way we interact with computer \t 0.5886733385226135\n",
      "222 \t way to apply latent semantic analysis on largecorpus text on macos terminal jupyterlab and \t 0.5861698083930247\n",
      "166 \t way to apply latent semantic analysis on largecorpus text on macos terminal jupyterlab and \t 0.5861698083930247\n",
      "11 \t way to apply latent semantic analysis on largecorpus text on macos terminal jupyterlab and \t 0.5861698083930247\n",
      "136 \t machine learning be fun part adam geitgey medium \t 0.5845320574900249\n",
      "98 \t machine learning be fun part adam geitgey medium \t 0.5845320574900249\n",
      "58 \t machine learning be fun part adam geitgey medium \t 0.5845320574900249\n",
      "18 \t machine learning be fun part adam geitgey medium \t 0.5845320574900249\n",
      "33 \t multiindex locality sensitive hash for fun and profit \t 0.5844512231711111\n",
      "29 \t system architecture for personalization and recommendation \t 0.5841850594956653\n",
      "213 \t how to easily detect object with deep learning on raspberry pi \t 0.5833461779613595\n",
      "328 \t how to easily detect object with deep learning on raspberry pi \t 0.5833461779613595\n",
      "269 \t how to easily detect object with deep learning on raspberry pi \t 0.5833461779613595\n",
      "221 \t how to easily detect object with deep learning on raspberry pi \t 0.5833461779613595\n",
      "278 \t how to easily detect object with deep learning on raspberry pi \t 0.5833461779613595\n",
      "206 \t how to build your own alphazero ai use python and kera \t 0.5814260014667536\n",
      "80 \t how to build your own alphazero ai use python and kera \t 0.5814260014667536\n",
      "55 \t how airbnb use machine learning to detect host preference \t 0.5803128551506108\n",
      "102 \t look for a ghost in the machine weird thing \t 0.5780627303534637\n",
      "265 \t turn your raspberry pi into homemade google home become human artificial intelligence magazine \t 0.5779175927364317\n",
      "203 \t machine learning for human part supervise learning \t 0.577769036140785\n",
      "21 \t data mining handling miss value the database developerzen \t 0.5765500162559998\n",
      "101 \t how to build a simple neural network in line of python code \t 0.5762627762977768\n",
      "126 \t how to build a simple neural network in line of python code \t 0.5762627762977768\n",
      "120 \t how I track my house movement use ibeacon universal mind medium \t 0.5760029178349353\n",
      "217 \t the beginner guide to conversational commerce the startup medium \t 0.5741243021896866\n",
      "237 \t she be our space odyssey jorge camacho medium \t 0.5704384281175303\n",
      "268 \t the new moat greylock perspective \t 0.5688144100408774\n",
      "132 \t start automate your business task with slack howdy \t 0.5684840683796266\n",
      "235 \t siris descendant how intelligent assistant will evolve \t 0.5670314720868466\n",
      "44 \t machine learning exercise in python part john wittenauer medium \t 0.5659725827522605\n",
      "306 \t why turing legacy demand a smart keyboard dr ben medlock medium \t 0.5655296618587944\n",
      "104 \t the technical trouble with humanoid robot weird thing \t 0.5640103236008813\n",
      "14 \t machine learning be fun part deep learning and convolutional neural network \t 0.563906382719779\n",
      "56 \t machine learning be fun part deep learning and convolutional neural network \t 0.563906382719779\n",
      "169 \t machine learning be fun part deep learning and convolutional neural network \t 0.563906382719779\n",
      "304 \t thank so much for your response jar really glad you enjoy read it \t 0.5636314308342361\n",
      "24 \t netflix recommendation beyond the star part \t 0.5622807197269962\n",
      "25 \t netflix recommendation beyond the star part \t 0.5610616078002424\n",
      "256 \t tensorflow in a nutshell part three all the model \t 0.5594488785428391\n",
      "232 \t the evolution a simple illustration leethree on ux medium \t 0.5585212436727511\n",
      "255 \t the future of digital banking k product design medium \t 0.5575923601834335\n",
      "298 \t classify website with neural network knowledge from datum the datafiniti blog \t 0.5566523767304294\n",
      "112 \t classify website with neural network knowledge from datum the datafiniti blog \t 0.5566523767304294\n",
      "31 \t classify website with neural network knowledge from datum the datafiniti blog \t 0.5566523767304294\n",
      "13 \t machine learning be fun adam geitgey medium \t 0.555432450849807\n",
      "36 \t machine learning be fun adam geitgey medium \t 0.555432450849807\n",
      "260 \t the great deep learning box assembly setup and benchmark \t 0.5543946297477481\n",
      "175 \t the great deep learning box assembly setup and benchmark \t 0.5543946297477481\n",
      "201 \t the great deep learning box assembly setup and benchmark \t 0.5543946297477481\n",
      "226 \t the great deep learning box assembly setup and benchmark \t 0.5543946297477481\n",
      "149 \t the great deep learning box assembly setup and benchmark \t 0.5543946297477481\n",
      "277 \t how to do semantic segmentation use deep learning \t 0.5516869354224384\n",
      "174 \t understand hinton capsule network part I intuition \t 0.550346985015557\n",
      "148 \t understand hinton capsule network part I intuition \t 0.550346985015557\n",
      "200 \t understand hinton capsule network part I intuition \t 0.550346985015557\n",
      "115 \t website morphing and more revolution in market arjan haring 🔮 🔨 medium \t 0.5473445853806861\n",
      "285 \t principal component analysis network in tensorflow with interactive code \t 0.5430663440787309\n",
      "49 \t the good data science and machine learning podcast \t 0.5428528676625048\n",
      "323 \t text classification use neural network machine learning \t 0.5419776446151807\n",
      "129 \t I let ibms robot chef tell I what to cook for a week \t 0.5384793822923271\n",
      "240 \t I let ibms robot chef tell I what to cook for a week \t 0.5384793822923271\n",
      "320 \t romance novel generate by artificial intelligence \t 0.5337299735070091\n",
      "296 \t romance novel generate by artificial intelligence \t 0.5337299735070091\n",
      "52 \t trick I learn from the otto kaggle challenge christophe bourguignat medium \t 0.5324008378831953\n",
      "69 \t how hbos silicon valley build \" not hotdog with mobile tensorflow keras react native \t 0.532307290332585\n",
      "145 \t how hbos silicon valley build \" not hotdog with mobile tensorflow keras react native \t 0.532307290332585\n",
      "197 \t how hbos silicon valley build \" not hotdog with mobile tensorflow keras react native \t 0.532307290332585\n",
      "79 \t amazing machine learning project for the past year v \t 0.5319025312635615\n",
      "317 \t traffic sign recognition with tensorflow waleed abdulla medium \t 0.5311975551522868\n",
      "168 \t beethoven picasso and artificial intelligence towards data science \t 0.5291366514330125\n",
      "193 \t simple reinforcement learning with tensorflow part asynchronous actorcritic agent ac \t 0.5284702681591896\n",
      "295 \t simple reinforcement learning with tensorflow part asynchronous actorcritic agent ac \t 0.5284702681591896\n",
      "310 \t simple reinforcement learning with tensorflow part asynchronous actorcritic agent ac \t 0.5284702681591896\n",
      "326 \t neural network architecture towards data science \t 0.5258471712966559\n",
      "6 \t an intro to machine learning for designer ux collective \t 0.523489126888615\n",
      "90 \t an intro to machine learning for designer ux collective \t 0.523489126888615\n",
      "271 \t understand capsule network ais allure new architecture \t 0.5228942112579164\n",
      "297 \t pick a gpu for deep learning slav \t 0.5226162973097119\n",
      "322 \t pick a gpu for deep learning slav \t 0.5226162973097119\n",
      "118 \t will all musician become robot roland trimmel medium \t 0.5172652398218148\n",
      "214 \t how you can train an ai to convert your design mockup into html and css \t 0.5145734142481488\n",
      "77 \t a tour of the top algorithm for machine learning newbie \t 0.51394999533543\n",
      "313 \t understand lstm and its diagram ml review medium \t 0.5111672291716758\n",
      "253 \t continuous video classification with tensorflow inception and recurrent net \t 0.5074506013983873\n",
      "314 \t adventure in narrated reality artist and machine intelligence medium \t 0.5053823293226208\n",
      "40 \t build a smart home feed pinter engineering medium \t 0.5033708004850969\n",
      "2 \t automate feature engineering in python towards data science \t 0.5011954355207413\n",
      "146 \t how do spotify know you so well member feature story medium \t 0.4994711474685036\n",
      "70 \t how do spotify know you so well member feature story medium \t 0.4994711474685036\n",
      "96 \t the complete beginner guide to chatbots chatbots magazine \t 0.49927175553660696\n",
      "124 \t of comet and god in the make thaddeus howze medium \t 0.498843636021442\n",
      "82 \t amazing python project for the past year v \t 0.49494280811000974\n",
      "95 \t from ballerina to ai researcher part I buzzrobot \t 0.49251995421892164\n",
      "10 \t from ballerina to ai researcher part I buzzrobot \t 0.49251995421892164\n",
      "334 \t spike neural network the next generation of machine learning \t 0.48955879231681765\n",
      "128 \t no ui be the new ui the startup medium \t 0.4888157118913421\n",
      "43 \t explain the wolfram language chris jager medium \t 0.4882724183501742\n",
      "250 \t continuous online video classification with tensorflow inception and a raspberry pi \t 0.48195695171963354\n",
      "287 \t tensorflow dataset api for increase training speed of neural network \t 0.4781164575509728\n",
      "318 \t cheat sheet for ai neural network machine learn deep learn big datum \t 0.47399036840268294\n",
      "202 \t cheat sheet for ai neural network machine learn deep learn big datum \t 0.47399036840268294\n",
      "292 \t cheat sheet for ai neural network machine learn deep learn big datum \t 0.47399036840268294\n",
      "176 \t cheat sheet for ai neural network machine learn deep learn big datum \t 0.47399036840268294\n",
      "219 \t sneakpeek the savedroid crypto save app part your wish \t 0.4708884462199884\n",
      "270 \t explore deepfake hacker noon \t 0.46952172172382795\n",
      "164 \t why datum scientist love gaussian towards data science \t 0.4649942834313669\n",
      "238 \t why ai research love pacman tommy thompson medium \t 0.4645506654888769\n",
      "125 \t why ai research love pacman tommy thompson medium \t 0.4645506654888769\n",
      "42 \t model madly data engineering medium \t 0.46003129913454277\n",
      "244 \t your temporary instant disposable dreamhouse for the weekend \t 0.4578800228610107\n",
      "177 \t distribute neural network with gpu in the aws cloud \t 0.456442900326205\n",
      "294 \t understand activation function in neural network \t 0.4556853008457853\n",
      "319 \t understand activation function in neural network \t 0.4556853008457853\n",
      "94 \t do google duplex just pass the ture test lance ulanoff medium \t 0.4537674655693566\n",
      "155 \t do google duplex just pass the ture test lance ulanoff medium \t 0.4537674655693566\n",
      "34 \t color base object segmentation akash shende medium \t 0.45339043278901814\n",
      "106 \t formalize indirect normativity ai alignment \t 0.4508134108841886\n",
      "27 \t formalize indirect normativity ai alignment \t 0.4508134108841886\n",
      "315 \t how to build a neuron explore ai in javascript pt \t 0.4448502327436238\n",
      "284 \t activation function neural network towards data science \t 0.4410901947675311\n",
      "186 \t compare artificial artist kyle mcdonald medium \t 0.4398779311769332\n",
      "5 \t intuitively understand convolution for deep learning \t 0.4377942687832942\n",
      "89 \t intuitively understand convolution for deep learning \t 0.4377942687832942\n",
      "158 \t intuitively understand convolution for deep learning \t 0.4377942687832942\n",
      "245 \t c plays bejewel blitz idanscott medium \t 0.42546875787829236\n",
      "127 \t facebook m the antituring test arik blog \t 0.41983550916118684\n",
      "252 \t a rock album for ai carlos beltran medium \t 0.411834724676007\n",
      "150 \t mindblowing implication of a driverless future \t 0.4058693124849963\n",
      "305 \t craig use neural network to learn mario nikolai savas medium \t 0.40223619739600663\n",
      "122 \t use oob tag in aiml part I pandorabotsblog medium \t 0.3981977747447645\n",
      "187 \t highway network with tensorflow jim fleme medium \t 0.3970047151745234\n",
      "242 \t announce poncho the weatherbot renderfrombetawork \t 0.384589579979273\n",
      "178 \t gradient descent vs coordinate descent hacker noon \t 0.3842463382339769\n",
      "180 \t load a tensorflow graph with the c api jim fleme medium \t 0.3693836963759633\n",
      "72 \t software andrej karpathy medium \t 0.36640673130735973\n",
      "249 \t everyday ia louis rosenfeld medium \t 0.36353517354720444\n",
      "53 \t machine learn เรียนอะไร รู้ไปทําไม o v e r f I t t e d medium \t 0.35609644158476983\n",
      "26 \t x fast spelling correction algorithm wolf garbe medium \t 0.3467468761693371\n",
      "50 \t tensorflow tutorial part illia polosukhin medium \t 0.34343634914403115\n",
      "303 \t take kera to the zoo gab \t 0.3303862983256246\n",
      "130 \t selfdrive car and the trolley problem tanay jaipuria medium \t 0.32556556861653774\n",
      "239 \t digital companionship matt wiese medium \t 0.3238898513566153\n",
      "276 \t coursera vs udacity for machine learning hacker noon \t 0.3169638364435906\n",
      "184 \t tensorflow tutorial part illia polosukhin medium \t 0.2960597848653113\n",
      "311 \t rohan lenny neural network the backpropagation algorithm explain \t 0.26798178931369815\n",
      "167 \t o grupo de estudo em deep learning de brasília está planejando o próximo ciclo de encontros do \t 0.05514148572190921\n",
      "74 \t artwork personalization at netflix netflix techblog medium \t 0.05441264992943981\n",
      "234 \t de la coopération entre les homme et les machine pour une approche pairàpair de lintelligence \t 0.05040155625015436\n",
      "286 \t multistream rnn concat rnn internal conv rnn lag rnn in tensorflow \t -0.013867945154564854\n",
      "307 \t semántica desde información desestructurada beeva labs \t -0.03530662036355521\n",
      "288 \t иипсихопат и ииобманщик hey machine learning \t -0.062405220094475\n",
      "23 \t 建议的程序员学习lda算法的步骤 蒸汽与魔法 \t -0.10395284412642974\n"
     ]
    }
   ],
   "source": [
    "titles = np.array(data[:,0]).reshape(-1, 1)\n",
    "texts = np.array(data[:,1]).reshape(-1, 1)\n",
    "indexes = np.argsort(scores)[::-1]\n",
    "scores_sorted = scores[indexes].reshape(-1, 1)\n",
    "indexes_T = indexes.reshape(-1, 1)\n",
    "\n",
    "sorted_data = np.concatenate((indexes_T, titles[indexes], texts[indexes], scores_sorted), axis=1)\n",
    "print(\"Index: \\t Title: \\t Score:\")\n",
    "for article in sorted_data:\n",
    "    print(article[0], \"\\t\", article[1], \"\\t\", article[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus - Našeptávání\n",
    "\n",
    "Bonusem dnešního cvičení je našeptávání pomocí rekurentních neuronových sítí. Úkolem je vytvořit jednoduchou rekurentní neuronovou síť, která bude generovat text (character-level přístup). \n",
    "\n",
    "Optimální je začít po dokončení cvičení k předmětu ANS, kde se tato úloha řeší. \n",
    "\n",
    "Dataset pro učení vaší neuronové sítě naleznete na stránkách [Yahoo research](https://webscope.sandbox.yahoo.com/catalog.php?datatype=l&guccounter=1), lze využít např. i větší [Kaggle dataset](https://www.kaggle.com/c/yandex-personalized-web-search-challenge/data) nebo vyhledat další dataset na [Google DatasetSearch](https://datasetsearch.research.google.com/).\n",
    "\n",
    "Vstupem bude rozepsaný dotaz a výstupem by měly být alespoň 3 dokončené dotazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
